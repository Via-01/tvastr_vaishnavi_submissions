# -*- coding: utf-8 -*-
"""ml-spark-vb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1xSSzWDcq6RQj0hNMdEpynbu7AcU9HL
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

"""# Overview"""

df = pd.read_csv("train.csv")
dft = pd.read_csv("test.csv")

id = dft['id']
id = list(id)

df.shape,dft.shape

df.head()

df.tail()

df.describe()

df.info()

"""# EDA"""

numeric_cols = df.select_dtypes(include=["number"]).columns
categorical_cols = df.select_dtypes(include=["object"]).columns

# Visualizing Distributions of Numerical Features
plt.figure(figsize=(12, 8))
df[numeric_cols].hist(bins=30, figsize=(12, 8), layout=(3, 3))
plt.suptitle("Distribution of Numerical Features")
plt.show()

plt.figure(figsize=(12, 8))
for i, col in enumerate(numeric_cols[:9]):
    plt.subplot(3, 3, i+1)
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()

import math

# Assuming df is already defined
categorical_cols = df.select_dtypes(include='object').columns

# Calculate the number of rows needed for the grid
n_cols = len(categorical_cols)
n_rows = math.ceil(n_cols / 3)

# Create the plot
fig, axes = plt.subplots(n_rows, 3, figsize=(20, 6*n_rows))
fig.suptitle('Countplots of Categorical Variables', fontsize=16)

for i, col in enumerate(categorical_cols):
    row = i // 3
    col_pos = i % 3
    ax = axes[row, col_pos] if n_rows > 1 else axes[col_pos]

    sns.countplot(y=df[col], order=df[col].value_counts().index, ax=ax)
    ax.set_title(f'Countplot of {col}')
    ax.set_xlabel('Count')
    ax.set_ylabel(col)

# Remove any unused subplots
for i in range(n_cols, n_rows * 3):
    row = i // 3
    col_pos = i % 3
    fig.delaxes(axes[row, col_pos] if n_rows > 1 else axes[col_pos])

plt.tight_layout()
plt.subplots_adjust(top=0.95)  # Adjust to make room for suptitle
plt.show()

import math

# Calculate the number of subplots needed
n_cats = len(categorical_cols)
n_nums = len(numeric_cols)
n_plots = n_cats + n_nums + 1  # +1 for target distribution
n_cols = 3
n_rows = math.ceil(n_plots / n_cols)

# Create the main figure and subplots
fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 6*n_rows))
fig.suptitle('Data Visualization', fontsize=16)
axs = axs.flatten()  # Flatten the 2D array of axes for easier indexing

# Target Variable Distribution
sns.countplot(x='Target', data=df, ax=axs[0])
axs[0].set_title('Target Variable Distribution')
print(df['Target'].value_counts(normalize=True))
axs[0].text(0.5, -0.1, f"Target Distribution:\n{df['Target'].value_counts(normalize=True).to_string()}",
            ha='center', va='center', transform=axs[0].transAxes)

# Categorical Variables vs Target
for i, cat_col in enumerate(categorical_cols):
    sns.countplot(x=cat_col, hue='Target', data=df, ax=axs[i+1])
    axs[i+1].set_xticklabels(axs[i+1].get_xticklabels(), rotation=90)
    axs[i+1].set_title(f'{cat_col} vs Target')
    axs[i+1].legend(title='Target', loc='upper right')

# Numerical Variables vs Target (Boxplots)
for i, num_col in enumerate(numeric_cols):
    sns.boxplot(x='Target', y=num_col, data=df, ax=axs[i+1+n_cats])
    axs[i+1+n_cats].set_title(f'Target vs {num_col}')

# Remove unused subplots
for i in range(n_plots, n_rows * n_cols):
    fig.delaxes(axs[i])

plt.tight_layout()
plt.subplots_adjust(top=0.95)  # Adjust for the suptitle
plt.show()

# prompt: pie chart of target distribution

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame and it contains a 'Target' column
target_counts = df['Target'].value_counts()
plt.figure(figsize=(8, 6))
plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Target Distribution')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

from sklearn.preprocessing import LabelEncoder
train_encoded = df.copy()
label_enc = LabelEncoder()

for col in categorical_cols:
    train_encoded[col] = label_enc.fit_transform(df[col])

plt.figure(figsize=(12, 6))
sns.heatmap(train_encoded[numeric_cols].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.show()

"""# PreProcessing"""

df = pd.read_csv("train.csv")
dft = pd.read_csv("test.csv")

def handle_unknown_values(df):
    """
    Handles 'unknown' values in specific columns of the dataset.
    - Keeps 'unknown' as a separate category for 'job' and 'education'.
    - Replaces 'unknown' in 'contact' with the most frequent value.
    - Converts 'poutcome' into a binary feature indicating whether it was known.

    Args:
        df (pd.DataFrame): Input DataFrame.

    Returns:
        pd.DataFrame: Processed DataFrame with unknown values handled.
    """

    # Replace 'unknown' in 'contact' with the most frequent category (mode)
    most_frequent_contact = df[df["contact"] != "unknown"]["contact"].mode()[0]
    df["contact"] = df["contact"].replace("unknown", most_frequent_contact)

    # Convert 'poutcome' to a binary feature (1 if known, 0 if unknown)
    df["poutcome_known"] = (df["poutcome"] != "unknown").astype(int)

    # Drop original 'poutcome' column
    df = df.drop(columns=["poutcome"])

    return df

# Apply to both df (training data) and dft (unseen test data)
df = handle_unknown_values(df)
dft = handle_unknown_values(dft)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

def create_preprocessing_pipeline(df, dft, target_col):
    """
    Creates a preprocessing pipeline for a DataFrame, including:
    - One-hot encoding for categorical columns.
    - Standard scaling for all numerical columns.

    Args:
        df (pd.DataFrame): The input DataFrame.
        target_col (str): The name of the target column.

    Returns:
        tuple: A tuple containing:
            - The preprocessed training data (X_train_transformed).
            - The preprocessed test data (X_test_transformed).
            - The target variable for training (y_train).
            - The target variable for testing (y_test).
            - The ColumnTransformer used.
    """

    # Separate features and target
    X = df.drop(target_col, axis=1)
    y = df[target_col]

    # Identify categorical and numerical columns
    categorical_cols = X.select_dtypes(include='object').columns.tolist()
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

    # Create transformers
    numerical_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown='ignore')  #handle_unknown avoids errors when new categories are seen at test time.

    # Create column transformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])

    # Create pipeline
    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Fit and transform the training data
    X_train_transformed = pipeline.fit_transform(X_train)

    # Transform the test data
    X_test_transformed = pipeline.transform(X_test)

    #Test set
    xdft = pipeline.transform(dft)

    return X_train_transformed, X_test_transformed, y_train, y_test, preprocessor, xdft

xtrain,xtest,ytrain,ytest,pipe,xdft = create_preprocessing_pipeline(df, dft, "Target")

"""# Model Training"""

from sklearn.dummy import DummyClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import numpy as np

def evaluate_models(xtrain, ytrain, xtest, ytest):
    """
    Evaluates multiple models and prints their F1-score, classification report, and confusion matrix.

    Models included:
    - Dummy Classifier (Baseline)
    - Logistic Regression
    - Random Forest
    - Gradient Boosting
    - Support Vector Machine (SVM)
    - XGBoost
    - LightGBM
    - CatBoost

    Args:
        xtrain (array-like): Training features.
        ytrain (array-like): Training target.
        xtest (array-like): Testing features.
        ytest (array-like): Testing target.

    Returns:
        dict: A dictionary containing trained models.
    """

    # Compute class weights for imbalanced dataset
    unique_classes, class_counts = np.unique(ytrain, return_counts=True)
    class_weight_ratio = {cls: max(class_counts) / count for cls, count in zip(unique_classes, class_counts)}

    models = {
        "Dummy Classifier": DummyClassifier(strategy="most_frequent"),
        "Logistic Regression": LogisticRegression(max_iter=1000, class_weight="balanced"),
        "Random Forest": RandomForestClassifier(random_state=42, class_weight="balanced"),
        "Gradient Boosting": GradientBoostingClassifier(random_state=42),
        "SVM": SVC(random_state=42, class_weight="balanced"),
        "XGBoost": XGBClassifier(random_state=42, scale_pos_weight=class_weight_ratio[1]),
        "LightGBM": LGBMClassifier(is_unbalance=True,random_state=42),
    }

    trained_models = {}

    for name, model in models.items():
        model.fit(xtrain, ytrain)
        y_pred = model.predict(xtest)

        print(f"\n{name} Results:")
        print(f"F1_Score: {f1_score(ytest, y_pred, average='weighted')}")
        print("Classification Report:\n", classification_report(ytest, y_pred))
        print("Confusion Matrix:\n", confusion_matrix(ytest, y_pred))

        trained_models[name] = model

    return trained_models

# Evaluate models
models = evaluate_models(xtrain, ytrain, xtest, ytest)

from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import f1_score

# Base Models
xgb = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05, random_state=42)
rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight="balanced")
gb = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)
svm = SVC(probability=True, random_state=42)  # Needed for soft voting
lr = LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42)

### 1Ô∏è‚É£ Voting Ensemble (Combines Predictions) ###
# Hard Voting: Majority Class Voting
hard_voting = VotingClassifier(
    estimators=[("XGB", xgb), ("RF", rf), ("GB", gb), ("SVM", svm)],
    voting="hard"
)

# Soft Voting: Weighted Probabilities Voting
soft_voting = VotingClassifier(
    estimators=[("XGB", xgb), ("RF", rf), ("GB", gb), ("SVM", svm)],
    voting="soft"
)

### 2Ô∏è‚É£ Stacking Ensemble (Learns from Base Models) ###
stacking = StackingClassifier(
    estimators=[("XGB", xgb), ("RF", rf), ("GB", gb)],
    final_estimator=LogisticRegression(),  # Meta-model
    passthrough=True,  # Uses raw + base predictions
)

### 3Ô∏è‚É£ Blending Ensemble (Weighted Averaging) ###
class BlendingEnsemble:
    def __init__(self, models, weights=None):
        self.models = models
        self.weights = weights if weights else [1 / len(models)] * len(models)

    def fit(self, X, y):
        for model in self.models:
            model.fit(X, y)

    def predict(self, X):
        preds = np.zeros((X.shape[0], len(self.models)))
        for i, model in enumerate(self.models):
            preds[:, i] = model.predict(X)
        return np.round(np.average(preds, axis=1, weights=self.weights)).astype(int)

# Initialize Blending Model
blending = BlendingEnsemble([xgb, rf, gb], weights=[0.4, 0.3, 0.3])

### üöÄ Train & Evaluate ###
ensembles = {
    "Hard Voting": hard_voting,
    "Soft Voting": soft_voting,
    "Stacking": stacking,
    "Blending": blending
}

for name, model in ensembles.items():
    model.fit(xtrain, ytrain)
    y_pred = model.predict(xtest)
    f1 = f1_score(ytest, y_pred, average="weighted")
    print(f"{name} F1-Score: {f1:.4f}")

model = models['XGBoost']
predictions = model.predict(xdft)

submission_df = pd.DataFrame({"id": id, "TARGET": predictions})
filename = "submission.csv"
submission_df.to_csv(filename, index=False)

print(f"Submission file '{filename}' created successfully.")

"""# Training Summary and Insights"""

# Assuming 'df' is your DataFrame and it contains a 'Target' column
# Replace 'categorical_cols' with the actual list of categorical column names if different
categorical_cols = df.select_dtypes(include='object').columns

num_plots = len(categorical_cols)
n_cols = 3
n_rows = (num_plots + n_cols - 1) // n_cols  # Calculate the number of rows

fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the array for easier indexing

for i, col in enumerate(categorical_cols):
    if i < num_plots:
      sns.countplot(x=col, hue='Target', data=df, ax=axes[i])
      axes[i].set_title(f'{col} vs Target')
      axes[i].tick_params(axis='x', rotation=45)
    else:
      # Hide any extra subplots if the number of categorical features is less than the grid size
      axes[i].set_visible(False)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

def plot_feature_importance(model, feature_names, model_name="Model"):
    """
    Plots the top 10 most important features from a trained model.

    Args:
        model: Trained model (e.g., XGBoost or Random Forest).
        feature_names: List of feature names.
        model_name: Name of the model for title.
    """
    # Extract feature importance
    feature_importance = model.feature_importances_

    # Get top 10 features
    top_n = 10
    top_indices = np.argsort(feature_importance)[-top_n:]  # Indices of top 10 features
    top_features = np.array(feature_names)[top_indices]
    top_importance = feature_importance[top_indices]

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    plt.barh(top_features, top_importance, color="skyblue")
    plt.xlabel("Feature Importance Score")
    plt.ylabel("Features")
    plt.title(f"Top 10 Most Important Features ({model_name})")
    plt.gca().invert_yaxis()  # Highest importance at the top
    plt.show()

# Get trained models from trained_models dictionary
xgb_model = models["XGBoost"]
rf_model = models["Random Forest"]

# Get feature names from preprocessing pipeline
feature_names = pipe.get_feature_names_out()

# Plot feature importance for XGBoost
plot_feature_importance(models['XGBoost'], feature_names, model_name="XGBoost")

import matplotlib.pyplot as plt
import numpy as np

# Updated model names and F1 scores
models = [
    "Dummy Classifier", "Logistic Regression", "Random Forest",
    "Gradient Boosting", "SVM", "XGBoost", "LightGBM",
    "Hard Voting", "Soft Voting", "Stacking", "Blending"
]

f1_scores = [
    0.8233, 0.8422, 0.8693, 0.8837, 0.8532, 0.8787, 0.8930,
    0.8859, 0.8892, 0.8885, 0.8941
]

# Sort models by F1-score in descending order
sorted_indices = np.argsort(f1_scores)[::-1]
models_sorted = [models[i] for i in sorted_indices]
f1_scores_sorted = [f1_scores[i] for i in sorted_indices]

# üìä Horizontal Bar Chart
plt.figure(figsize=(12, 6))
plt.barh(models_sorted, f1_scores_sorted, color="royalblue")
plt.xlabel("F1 Score")
plt.ylabel("Models")
plt.title("F1 Scores of Different Models")
plt.xlim(min(f1_scores_sorted) - 0.01, max(f1_scores_sorted) + 0.01)  # Dynamic x-axis
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.gca().invert_yaxis()  # Invert for highest at the top
plt.show()

# üìà Line Chart for Trend Analysis
plt.figure(figsize=(12, 6))
plt.plot(models_sorted, f1_scores_sorted, marker="o", linestyle="-", color="b", label="F1 Score")
plt.xticks(rotation=45, ha="right")
plt.xlabel("Models")
plt.ylabel("F1 Score")
plt.title("F1 Score Comparison Across Models")
plt.ylim(min(f1_scores_sorted) - 0.01, max(f1_scores_sorted) + 0.01)  # Dynamic y-axis
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.legend()
plt.show()

# prompt: line chart comparing recall of class 1 of dummy classifier with the other models?

import matplotlib.pyplot as plt

# Assuming 'models' dictionary contains the trained models, including 'Dummy Classifier'
# and 'ytest' is the true target variable for the test set.
# Replace with your actual model evaluation results.

dummy_recall = 0.5  # Replace with the actual recall for the dummy classifier (class 1)
model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM', 'XGBoost', 'LightGBM']
model_recalls = [0.6, 0.7, 0.8, 0.75, 0.85, 0.82]  # Replace with the actual recall values for class 1


plt.figure(figsize=(10, 6))
plt.plot(model_names, model_recalls, marker='o', label='Other Models')
plt.axhline(y=dummy_recall, color='r', linestyle='--', label='Dummy Classifier')
plt.xlabel("Model")
plt.ylabel("Recall of Class 1")
plt.title("Recall Comparison of Class 1")
plt.xticks(rotation=45, ha="right")
plt.legend()
plt.tight_layout()
plt.show()